import os
import feedparser
import requests
import tweepy
from google import genai
from dotenv import load_dotenv

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ (Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù„ÙˆÚ©Ø§Ù„)
load_dotenv()

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ù„ÛŒØ¯Ù‡Ø§ (Ø§ÛŒÙ†â€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ .env ÛŒØ§ GitHub Secrets Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯) ---
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

# Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ ØªÙˆÛŒÛŒØªØ± (X)
X_API_KEY = os.environ.get("X_API_KEY")
X_API_SECRET = os.environ.get("X_API_SECRET")
X_ACCESS_TOKEN = os.environ.get("X_ACCESS_TOKEN")
X_ACCESS_SECRET = os.environ.get("X_ACCESS_SECRET")

# --- Û±. Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² RSS ---
def get_news():
    print("ğŸŒ Scanning for updates...")
    rss_url = "https://news.google.com/rss/search?q=schengen+visa+rules+2026+OR+european+residency+investment&hl=en-US&gl=US&ceid=US:en"
    feed = feedparser.parse(rss_url)
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±
    if os.path.exists("history.txt"):
        with open("history.txt", "r") as f:
            history = f.read().splitlines()
    else:
        history = []

    for entry in feed.entries:
        if entry.link not in history:
            return entry # Ø§ÙˆÙ„ÛŒÙ† Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
    return None

# --- Û². Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Gemini ---
def generate_content(news_entry):
    print("ğŸ¤– AI is analyzing with Gemini...")
    client = genai.Client(api_key=GEMINI_API_KEY)
    
    prompt = f"""
    Analyze this news: {news_entry.title}
    Link: {news_entry.link}
    
    Task: Create a professional summary for immigration and investment interests.
    Output MUST be in this exact format:
    TELEGRAM: (A catchy title and 3 bullet points in Persian)
    X_POST: (A short, engaging English tweet with hashtags, max 240 chars)
    """
    
    try:
        response = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
        text = response.text
        
        # Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… Ùˆ ØªÙˆÛŒÛŒØªØ±
        parts = text.split("X_POST:")
        telegram_part = parts[0].replace("TELEGRAM:", "").strip()
        x_part = parts[1].strip() if len(parts) > 1 else ""
        
        return {"telegram": telegram_part, "x": x_part}
    except Exception as e:
        print(f"âŒ Gemini Error: {e}")
        return None

# --- Û³. Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙˆÛŒÛŒØªØ± (X) ---
def post_to_x(tweet_text):
    print("ğŸ¦ Posting to X (Twitter)...")
    try:
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Tweepy Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ù¾Ø³Øª
        client_x = tweepy.Client(
            consumer_key=X_API_KEY,
            consumer_secret=X_API_SECRET,
            access_token=X_ACCESS_TOKEN,
            access_token_secret=X_ACCESS_SECRET
        )
        client_x.create_tweet(text=tweet_text)
        print("âœ… Posted to X successfully!")
    except Exception as e:
        print(f"âŒ X API Error: {e}")

# --- Û´. Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù… ---
def send_telegram(text, link):
    print("ğŸ“¢ Sending to Telegram...")
    message = f"{text}\n\nğŸ”— Source: {link}"
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    requests.post(url, data={"chat_id": TELEGRAM_CHAT_ID, "text": message, "parse_mode": "HTML"})

# --- Ûµ. Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡ ---
def save_history(link):
    with open("history.txt", "a") as f:
        f.write(link + "\n")

# --- Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ ---
if __name__ == "__main__":
    news = get_news()
    if news:
        ai_content = generate_content(news)
        if ai_content:
            # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
            send_telegram(ai_content['telegram'], news.link)
            
            # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙˆÛŒÛŒØªØ±
            if ai_content['x']:
                post_to_x(ai_content['x'])
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒÙ†Ú©
            save_history(news.link)
            print("ğŸ’¾ Done! Everything synchronized.")
    else:
        print("â˜• No new news found.")