import google.generativeai as genai
import feedparser
import json
import os
import requests
import re # Ø¨Ø±Ø§ÛŒ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ø®Ø±ÙˆØ¬ÛŒ Ø¬ÛŒØ³ÙˆÙ†

# ==========================================
# ğŸ”‘ Ú©Ù„ÛŒØ¯ API Ø®ÙˆØ¯Øª Ø±Ùˆ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø²Ø§Ø±
GEMINI_API_KEY = "AIzaSyBuiM0z6SlJpA_L1B_tdf9-8cFYJOYklS4".strip()
# ==========================================

HISTORY_FILE = "history.txt"
genai.configure(api_key=GEMINI_API_KEY)

# âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ùˆ Ø³Ø±ÛŒØ¹ 2.0 Flash Ú©Ù‡ Ø¯Ø± Ù„ÛŒØ³Øª Ø´Ù…Ø§ Ø¨ÙˆØ¯
model = genai.GenerativeModel('models/gemini-2.0-flash')

# --- ØªÙˆØ§Ø¨Ø¹ Ø­Ø§ÙØ¸Ù‡ ---
def load_history():
    if not os.path.exists(HISTORY_FILE):
        return []
    with open(HISTORY_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f.readlines()]

def save_to_history(link):
    with open(HISTORY_FILE, "a", encoding="utf-8") as f:
        f.write(f"{link}\n")

# --- Û±. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø®Ø¨Ø± ---
def get_news():
    print("ğŸŒ Acting like a browser to fetch news...")
    
    # Ù„ÛŒÙ†Ú© Ø§Ø®Ø¨Ø§Ø±
    rss_url = "https://news.google.com/rss/search?q=Europe+immigration+visa+rules&hl=en-US&gl=US&ceid=US:en"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    try:
        response = requests.get(rss_url, headers=headers, timeout=10)
        feed = feedparser.parse(response.content)
        
        if feed.entries:
            news_item = feed.entries[0]
            print(f"âœ… News Found: {news_item.title}")
            return {
                "title": news_item.title,
                "link": news_item.link,
                "summary": news_item.summary if 'summary' in news_item else news_item.title
            }
        else:
            print("âš ï¸ Google returned empty feed.")
            return None
            
    except Exception as e:
        print(f"âŒ Connection Error: {e}")
        return None

# --- Û². ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ ---
def generate_content(news_item):
    print("ğŸ¤– Gemini 2.0 is thinking...")
    
    prompt = f"""
    You are a social media expert.
    News Title: "{news_item['title']}"
    
    Task:
    1. Identify the country. If none, use "Europe ğŸ‡ªğŸ‡º".
    2. Write a short Twitter post (under 280 chars).
    3. Write a LinkedIn post (professional).
    
    IMPORTANT: Output ONLY valid JSON.
    Format:
    {{
        "twitter_draft": "Your tweet here",
        "linkedin_draft": "Your linkedin post here"
    }}
    """
    
    try:
        response = model.generate_content(prompt)
        text = response.text
        
        # ğŸ§¹ ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø§Ø±ÙˆØ± Ù†Ø¯Ù‡
        # Ú¯Ø§Ù‡ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ```json Ø§ÙˆÙ„Ø´ Ù…ÛŒØ²Ø§Ø±Ù†ØŒ Ø§ÛŒÙ† Ú©Ø¯ Ù¾Ø§Ú©Ø´ Ù…ÛŒÚ©Ù†Ù‡
        text = re.sub(r"```json\s*", "", text)
        text = re.sub(r"```", "", text)
        text = text.strip()
        
        return json.loads(text)
    except Exception as e:
        print(f"âŒ AI Parsing Error: {e}")
        print(f"Raw Output: {response.text if 'response' in locals() else 'No response'}")
        return None

# --- Ø§Ø¬Ø±Ø§ ---
if __name__ == "__main__":
    sent_links = load_history()
    news = get_news()
    
    if news:
        if news['link'] in sent_links:
            print("â›” Duplicate! We sent this already.")
        else:
            content = generate_content(news)
            if content:
                print("\n" + "="*30)
                # Ø§Ø² .get Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø§Ú¯Ø± Ú©Ù„ÛŒØ¯ Ù†Ø¨ÙˆØ¯ Ø§Ø±ÙˆØ± Ù†Ø¯Ù‡
                print("ğŸ¦ TWITTER:\n" + content.get('twitter_draft', 'No Tweet Generated'))
                print(f"\nğŸ”— {news['link']}")
                print("-" * 30)
                print("ğŸ’¼ LINKEDIN:\n" + content.get('linkedin_draft', 'No LinkedIn Post Generated'))
                print("="*30 + "\n")
                
                save_to_history(news['link'])
                print("ğŸ’¾ Saved to history.")
    else:
        print("ğŸ˜´ No news found.")